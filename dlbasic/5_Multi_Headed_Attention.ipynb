{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5_Multi_Headed_Attention.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNfN1AxfNVGLg9BVJgGa67F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G30dGbFpIz5S","executionInfo":{"status":"ok","timestamp":1644562593459,"user_tz":-540,"elapsed":5821,"user":{"displayName":"정승환","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05931280673532507641"}},"outputId":"c9f816f8-2e8a-4fa3-f8ab-6415369ada3f"},"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch version:[1.10.0+cu111].\n","device:[cuda:0].\n"]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","%matplotlib inline\n","%config InlineBackend.figure_format='retina'\n","print (\"PyTorch version:[%s].\"%(torch.__version__))\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print (\"device:[%s].\"%(device))"]},{"cell_type":"code","source":["# Scaled Dot-Product Attention\n","\n","class ScaledDotProductAttention(nn.Module):\n","    def forward(self, Q, K, V, mask=None):\n","        d_K = K.size()[-1]\n","        scores = Q.matmul(K.transpose(-2,-1)) / np.sqrt(d_K)\n","        if mask is not None :\n","            scores = scores.masked_fill(mask==0, -1e9)\n","        attention = F.softmax(scores, dim=-1)\n","        out = attention.matmul(V)\n","        return out, attention\n","    \n","SPDA = ScaledDotProductAttention()\n","n_batch, d_K, d_V = 3, 128, 256 # K != V 달라도 된다.\n","n_Q, n_K, n_V = 30, 50, 50 # Q != K 달라도 된다.\n","Q = torch.rand(n_batch, n_Q, d_K)\n","K = torch.rand(n_batch, n_K, d_K)\n","V = torch.rand(n_batch, n_V, d_V)\n","out, attention = SPDA.forward(Q,K,V,mask=None)\n","def sh(x) : return str(x.shape)[11:-1]\n","print(f\"SPDA | Q : {sh(Q)} | K : {sh(K)} | V : {sh(V)} | out : {sh(out)} | attention : {sh(attention)}\")\n","\n","n_batch, n_head, d_K, d_V = 3, 5, 128, 256\n","n_Q, n_K, n_V = 30, 50, 50\n","Q = torch.rand(n_batch, n_head, n_Q, d_K)\n","K = torch.rand(n_batch, n_head, n_K, d_K)\n","V = torch.rand(n_batch, n_head, n_V, d_V)\n","out, attention = SPDA.forward(Q,K,V,mask=None)\n","print(f\"(M)SPDA | Q : {sh(Q)} | K : {sh(K)} | V : {sh(V)} | out : {sh(out)} | attention : {sh(attention)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XSqOqD_dI6Lv","executionInfo":{"status":"ok","timestamp":1644562594251,"user_tz":-540,"elapsed":798,"user":{"displayName":"정승환","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05931280673532507641"}},"outputId":"2a578aed-2ba3-4d07-d0e9-e4b23cc2375a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["SPDA | Q : [3, 30, 128] | K : [3, 50, 128] | V : [3, 50, 256] | out : [3, 30, 256] | attention : [3, 30, 50]\n","(M)SPDA | Q : [3, 5, 30, 128] | K : [3, 5, 50, 128] | V : [3, 5, 50, 256] | out : [3, 5, 30, 256] | attention : [3, 5, 30, 50]\n"]}]},{"cell_type":"code","source":["#Multi head\n","\n","class MultiHeadedAttention(nn.Module):\n","    def __init__(self, d_feat=128, n_head=5, actv=F.relu, USE_BIAS=True, dropout_p=0.1, device=None):\n","        \"\"\"\n","        d_feat : feature dimension\n","        n_head : number of heads\n","        actv : activation after each linear layer\n","        USE_BIAS : whether use bias\n","        dropout_p : dropout rate\n","        device : which device to use\n","        \"\"\"\n","\n","        super(MultiHeadedAttention, self).__init__()\n","        if (d_feat%n_head) != 0 :\n","            raise ValueError(f\"d_feat({d_feat:d}) should be dibsible by b_head({n_head})\")\n","        self.d_feat = d_feat\n","        self.n_head = n_head\n","        self.d_head = self.d_feat // self.n_head\n","        self.actv = actv\n","        self.USE_BIAS = USE_BIAS\n","        self.dropout_p = dropout_p\n","\n","        self.lin_Q = nn.Linear(self.d_feat, self.d_feat, self.USE_BIAS)\n","        self.lin_K = nn.Linear(self.d_feat, self.d_feat, self.USE_BIAS)\n","        self.lin_V = nn.Linear(self.d_feat, self.d_feat, self.USE_BIAS)\n","        self.lin_O = nn.Linear(self.d_feat, self.d_feat, self.USE_BIAS)\n","\n","        self.dropout = nn.Dropout(p=self.dropout_p)\n","    \n","    def forward(self, Q, K, V, mask=None):\n","        \"\"\"\n","        param Q : [n_batch, n_Q, d_feat]\n","        param X : [n_batch, n_K, d_feat]\n","        param V : [n_batch, n_V, d_feat]\n","        param mask\n","        \"\"\"\n","        n_batch = Q.shape[0]\n","        Q_feat = self.lin_Q(Q)\n","        K_feat = self.lin_K(K)\n","        V_feat = self.lin_V(V)\n","\n","        # Multi-head split of Q,K,V\n","        Q_split = Q_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0,2,1,3)\n","        K_split = K_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0,2,1,3)\n","        V_split = V_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0,2,1,3)\n","\n","        # Multi-headed Attention\n","        scores = torch.matmul(Q_split, K_split.permute(0,1,3,2)) / np.sqrt(d_K)\n","        if mask is not None :\n","            scores = scores.masked_fill(mask==0, -1e9)\n","        attention = torch.softmax(scores, dim=-1)\n","        x_raw = torch.matmul(self.dropout(attention), V_split)\n","\n","        x_rsh1 = x_raw.permute(0,2,1,3).contiguous()\n","        x_rsh2 = x_rsh1.view(n_batch, -1, self.d_feat)\n","\n","        x = self.lin_O(x_rsh2)\n","        out = {'Q_feat'}"],"metadata":{"id":"cjYb3oVnN7TF","colab":{"base_uri":"https://localhost:8080/","height":131},"executionInfo":{"status":"error","timestamp":1644398928845,"user_tz":-540,"elapsed":326,"user":{"displayName":"정승환","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05931280673532507641"}},"outputId":"84f1f85c-334d-4ca8-d8fd-d0049d5ea592"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-6249df0d2c52>\"\u001b[0;36m, line \u001b[0;32m32\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"]}]}]}